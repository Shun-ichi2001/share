{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from  datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the correct data type for each column in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *calendar.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  wm_yr_wk  weekday  wday  month  year    d  event_name_1  \\\n",
       "0 2011-01-29     11101        2     1      1  2011  d_1             0   \n",
       "1 2011-01-30     11101        3     2      1  2011  d_2             0   \n",
       "2 2011-01-31     11101        1     3      1  2011  d_3             0   \n",
       "3 2011-02-01     11101        5     4      2  2011  d_4             0   \n",
       "4 2011-02-02     11101        6     5      2  2011  d_5             0   \n",
       "\n",
       "   event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0             0             0             0      0.0      0.0      0.0  \n",
       "1             0             0             0      0.0      0.0      0.0  \n",
       "2             0             0             0      0.0      0.0      0.0  \n",
       "3             0             0             0      1.0      1.0      0.0  \n",
       "4             0             0             0      1.0      0.0      1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct data types for \"calendar.csv\"\n",
    "calendarDTypes = {\"event_name_1\": \"category\", \n",
    "                  \"event_name_2\": \"category\", \n",
    "                  \"event_type_1\": \"category\", \n",
    "                  \"event_type_2\": \"category\", \n",
    "                  \"weekday\": \"category\", \n",
    "                  'wm_yr_wk': 'int16', \n",
    "                  \"wday\": \"int16\",\n",
    "                  \"month\": \"int16\", \n",
    "                  \"year\": \"int16\", \n",
    "                  \"snap_CA\": \"float32\", \n",
    "                  'snap_TX': 'float32', \n",
    "                  'snap_WI': 'float32' }\n",
    "\n",
    "# Read csv file\n",
    "# ここは変える\n",
    "calendar = pd.read_csv(\"data/calendar.csv\", \n",
    "                       dtype = calendarDTypes)\n",
    "\n",
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col, colDType in calendarDTypes.items():\n",
    "    if colDType == \"category\":\n",
    "        calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n",
    "        calendar[col] -= calendar[col].min()\n",
    "\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sell_prices.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id  item_id  wm_yr_wk  sell_price\n",
       "0         0        0     11325        9.58\n",
       "1         0        0     11326        9.58\n",
       "2         0        0     11327        8.26\n",
       "3         0        0     11328        8.26\n",
       "4         0        0     11329        8.26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct data types for \"sell_prices.csv\"\n",
    "priceDTypes = {\"store_id\": \"category\", \n",
    "               \"item_id\": \"category\", \n",
    "               \"wm_yr_wk\": \"int16\",\n",
    "               \"sell_price\":\"float32\"}\n",
    "\n",
    "# Read csv file\n",
    "prices = pd.read_csv(\"data/sell_prices.csv\", \n",
    "                     dtype = priceDTypes)\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col, colDType in priceDTypes.items():\n",
    "    if colDType == \"category\":\n",
    "        prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "        prices[col] -= prices[col].min()\n",
    "        \n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sales_train_validation.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *train.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-07</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_251</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  store_id  cat_id  \\\n",
       "0  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "1  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "2  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "3  HOBBIES_1_004_CA_1_evaluation        3        0         0       0   \n",
       "4  HOBBIES_1_004_CA_1_evaluation        3        0         0       0   \n",
       "\n",
       "   state_id      d  sales       date  wm_yr_wk  ...  month  year  \\\n",
       "0         0  d_250    0.0 2011-10-05     11136  ...     10  2011   \n",
       "1         0  d_251    0.0 2011-10-06     11136  ...     10  2011   \n",
       "2         0  d_252    0.0 2011-10-07     11136  ...     10  2011   \n",
       "3         0  d_250    0.0 2011-10-05     11136  ...     10  2011   \n",
       "4         0  d_251    4.0 2011-10-06     11136  ...     10  2011   \n",
       "\n",
       "   event_name_1  event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  \\\n",
       "0             0             0             0             0      1.0      1.0   \n",
       "1             0             0             0             0      1.0      1.0   \n",
       "2             0             0             0             0      1.0      1.0   \n",
       "3             0             0             0             0      1.0      1.0   \n",
       "4             0             0             0             0      1.0      1.0   \n",
       "\n",
       "   snap_WI  sell_price  \n",
       "0      1.0        3.97  \n",
       "1      1.0        3.97  \n",
       "2      0.0        3.97  \n",
       "3      1.0        4.34  \n",
       "4      1.0        4.34  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstDay = 250\n",
    "lastDay = 1913\n",
    "\n",
    "# Use x sales days (columns) for training\n",
    "numCols = [f\"d_{day}\" for day in range(firstDay, lastDay+1)]\n",
    "\n",
    "# Define all categorical columns\n",
    "catCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "\n",
    "# Define the correct data types for \"sales_train_validation.csv\"\n",
    "dtype = {numCol: \"float32\" for numCol in numCols} \n",
    "dtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n",
    "\n",
    "# Read csv file\n",
    "# ここ変える\n",
    "ds = pd.read_csv(\"data/trainset.csv\", \n",
    "                 usecols = catCols + numCols, dtype = dtype)\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col in catCols:\n",
    "    if col != \"id\":\n",
    "        ds[col] = ds[col].cat.codes.astype(\"int16\")\n",
    "        ds[col] -= ds[col].min()\n",
    "        \n",
    "ds = pd.melt(ds,\n",
    "             id_vars = catCols,\n",
    "             value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n",
    "             var_name = \"d\",\n",
    "             value_name = \"sales\")\n",
    "\n",
    "# Merge \"ds\" with \"calendar\" and \"prices\" dataframe\n",
    "ds = ds.merge(calendar, on = \"d\", copy = False)\n",
    "ds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加した特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    submission = submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "    print(\"submission shape:\", submission.shape)\n",
    "\n",
    "    # calendar shape: (1969, 14)\n",
    "    # sell_prices shape: (6841121, 4)\n",
    "    # sales_train_val shape: (30490, 1919)\n",
    "    # submission shape: (60980, 29)\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission shape: (30490, 20)\n"
     ]
    }
   ],
   "source": [
    "calendar\n",
    "sales = ds\n",
    "prices\n",
    "submission = read_data()\n",
    "\n",
    "NUM_ITEMS = ds.shape[0]  # 30490\n",
    "DAYS_PRED = submission.shape[1] - 1  # 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar = encode_categorical(\n",
    "    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "sales = encode_categorical(\n",
    "    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "prices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num(ser):\n",
    "    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n",
    "\n",
    "\n",
    "def reshape_sales(sales, submission, d_thresh=0, verbose=True):\n",
    "    # melt sales data, get it ready for training\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    # get product table.\n",
    "    product = sales[id_columns]\n",
    "\n",
    "    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n",
    "    sales = reduce_mem_usage(sales)\n",
    "\n",
    "    # separate test dataframes.\n",
    "    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    # change column names.\n",
    "    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n",
    "    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "\n",
    "    # merge with product table\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n",
    "    vals = vals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"validation\")\n",
    "        display(vals)\n",
    "\n",
    "        print(\"evaluation\")\n",
    "        display(evals)\n",
    "\n",
    "    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    sales[\"part\"] = \"train\"\n",
    "    vals[\"part\"] = \"validation\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "\n",
    "    data = pd.concat([sales, vals, evals], axis=0)\n",
    "\n",
    "    del sales, vals, evals\n",
    "\n",
    "    data[\"d\"] = extract_num(data[\"d\"])\n",
    "    data = data[data[\"d\"] >= d_thresh]\n",
    "\n",
    "    # delete evaluation for now.\n",
    "    data = data[data[\"part\"] != \"evaluation\"]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data\")\n",
    "        display(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_calendar(data, calendar):\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "\n",
    "def merge_prices(data, prices):\n",
    "    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.05 GiB for an array with shape (677962912,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m reshape_sales(sales, submission, d_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1941\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m365\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sales\n\u001b[0;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mreshape_sales\u001b[1;34m(sales, submission, d_thresh, verbose)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# get product table.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m product \u001b[38;5;241m=\u001b[39m sales[id_columns]\n\u001b[1;32m---> 12\u001b[0m sales \u001b[38;5;241m=\u001b[39m sales\u001b[38;5;241m.\u001b[39mmelt(id_vars\u001b[38;5;241m=\u001b[39mid_columns, var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemand\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m     13\u001b[0m sales \u001b[38;5;241m=\u001b[39m reduce_mem_usage(sales)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# separate test dataframes.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9636\u001b[0m, in \u001b[0;36mDataFrame.melt\u001b[1;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[0;32m   9626\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaller\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.melt(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m   9627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelt\u001b[39m(\n\u001b[0;32m   9628\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9634\u001b[0m     ignore_index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9635\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 9636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m melt(\n\u001b[0;32m   9637\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9638\u001b[0m         id_vars\u001b[38;5;241m=\u001b[39mid_vars,\n\u001b[0;32m   9639\u001b[0m         value_vars\u001b[38;5;241m=\u001b[39mvalue_vars,\n\u001b[0;32m   9640\u001b[0m         var_name\u001b[38;5;241m=\u001b[39mvar_name,\n\u001b[0;32m   9641\u001b[0m         value_name\u001b[38;5;241m=\u001b[39mvalue_name,\n\u001b[0;32m   9642\u001b[0m         col_level\u001b[38;5;241m=\u001b[39mcol_level,\n\u001b[0;32m   9643\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m   9644\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\melt.py:146\u001b[0m, in \u001b[0;36mmelt\u001b[1;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(var_name):\n\u001b[0;32m    144\u001b[0m     mdata[col] \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_level_values(i)\u001b[38;5;241m.\u001b[39mrepeat(N)\n\u001b[1;32m--> 146\u001b[0m result \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39m_constructor(mdata, columns\u001b[38;5;241m=\u001b[39mmcolumns)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_index:\n\u001b[0;32m    149\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m tile_compat(frame\u001b[38;5;241m.\u001b[39mindex, K)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:629\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    626\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[0;32m    627\u001b[0m     val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m--> 629\u001b[0m val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    630\u001b[0m com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    631\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\construction.py:608\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    606\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 608\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(data)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    610\u001b[0m         object_index\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[0;32m    613\u001b[0m     ):\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[0;32m    615\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1180\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[1;32m-> 1180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m     value,\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;66;03m#  numpy would have done it for us.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m     convert_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1185\u001b[0m     convert_non_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1186\u001b[0m     dtype_if_all_nat\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1187\u001b[0m )\n",
      "File \u001b[1;32mlib.pyx:2520\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.05 GiB for an array with shape (677962912,) and data type float64"
     ]
    }
   ],
   "source": [
    "data = reshape_sales(sales, submission, d_thresh=1941 - int(365 * 2))\n",
    "del sales\n",
    "gc.collect()\n",
    "\n",
    "calendar[\"d\"] = extract_num(calendar[\"d\"])\n",
    "data = merge_calendar(data, calendar)\n",
    "del calendar\n",
    "gc.collect()\n",
    "\n",
    "data = merge_prices(data, prices)\n",
    "del prices\n",
    "gc.collect()\n",
    "\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demand_features(df):\n",
    "    for diff in [0, 1, 2]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60, 90, 180]:\n",
    "        df[f\"rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60, 90, 180]:\n",
    "        df[f\"rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60]:\n",
    "        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60]:\n",
    "        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n",
    "        )\n",
    "\n",
    "    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n",
    "    )\n",
    "    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_features(df):\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n",
    "\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    attrs = [\n",
    "        \"year\",\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")\n",
    "\n",
    "print(\"start date:\", data[dt_col].min())\n",
    "print(\"end date:\", data[dt_col].max())\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTimeSeriesSplitter:\n",
    "    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n",
    "        self.n_splits = n_splits\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.day_col = day_col\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        SEC_IN_DAY = 3600 * 24\n",
    "        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n",
    "        duration = sec.max()\n",
    "\n",
    "        train_sec = self.train_days * SEC_IN_DAY\n",
    "        test_sec = self.test_days * SEC_IN_DAY\n",
    "        total_sec = test_sec + train_sec\n",
    "\n",
    "        if self.n_splits == 1:\n",
    "            train_start = duration - total_sec\n",
    "            train_end = train_start + train_sec\n",
    "\n",
    "            train_mask = (sec >= train_start) & (sec < train_end)\n",
    "            test_mask = sec >= train_end\n",
    "\n",
    "            yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "        else:\n",
    "            # step = (duration - total_sec) / (self.n_splits - 1)\n",
    "            step = DAYS_PRED * SEC_IN_DAY\n",
    "\n",
    "            for idx in range(self.n_splits):\n",
    "                # train_start = idx * step\n",
    "                shift = (self.n_splits - (idx + 1)) * step\n",
    "                train_start = duration - total_sec - shift\n",
    "                train_end = train_start + train_sec\n",
    "                test_end = train_end + test_sec\n",
    "\n",
    "                train_mask = (sec > train_start) & (sec <= train_end)\n",
    "\n",
    "                if idx == self.n_splits - 1:\n",
    "                    test_mask = sec > train_end\n",
    "                else:\n",
    "                    test_mask = (sec > train_end) & (sec <= test_end)\n",
    "\n",
    "                yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_col = \"d\"\n",
    "cv_params = {\n",
    "    \"n_splits\": 3,\n",
    "    \"train_days\": int(365 * 1.5),\n",
    "    \"test_days\": DAYS_PRED,\n",
    "    \"day_col\": day_col,\n",
    "}\n",
    "cv = CustomTimeSeriesSplitter(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cv_days(cv, X, dt_col, day_col):\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X)):\n",
    "        print(f\"----- Fold: ({ii + 1} / {cv.n_splits}) -----\")\n",
    "        tr_start = X.iloc[tr][dt_col].min()\n",
    "        tr_end = X.iloc[tr][dt_col].max()\n",
    "        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n",
    "\n",
    "        tt_start = X.iloc[tt][dt_col].min()\n",
    "        tt_end = X.iloc[tt][dt_col].max()\n",
    "        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"start\": [tr_start, tt_start],\n",
    "                \"end\": [tr_end, tt_end],\n",
    "                \"days\": [tr_days, tt_days],\n",
    "            },\n",
    "            index=[\"train\", \"test\"],\n",
    "        )\n",
    "\n",
    "        display(df)\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, dt_col, lw=10):\n",
    "    n_splits = cv.get_n_splits()\n",
    "    _, ax = plt.subplots(figsize=(20, n_splits))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            X[dt_col],\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=plt.cm.coolwarm,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    MIDDLE = 15\n",
    "    LARGE = 20\n",
    "    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n",
    "    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n",
    "    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n",
    "    ax.set_yticks(np.arange(n_splits) + 0.5)\n",
    "    ax.set_yticklabels(list(range(n_splits)))\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\n",
    "show_cv_days(cv, sample, dt_col, day_col)\n",
    "plot_cv_indices(cv, sample, dt_col)\n",
    "\n",
    "del sample\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayLags = [7, 28]\n",
    "lagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\n",
    "for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "    ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n",
    "    \n",
    "windows = [7, 28]\n",
    "for window in windows:\n",
    "    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "        ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFeatures = {\"wday\": \"weekday\",\n",
    "                \"week\": \"weekofyear\",\n",
    "                \"month\": \"month\",\n",
    "                \"quarter\": \"quarter\",\n",
    "                \"year\": \"year\",\n",
    "                \"mday\": \"day\"}\n",
    "\n",
    "for featName, featFunc in dateFeatures.items():\n",
    "    if featName in ds.columns:\n",
    "        ds[featName] = ds[featName].astype(\"int16\")\n",
    "    else:\n",
    "        ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFeatures = {\"item_id\",\n",
    "                \"dept_id\",\n",
    "                \"cat_id\",\n",
    "                \"store_id\",\n",
    "                \"state_id\",\n",
    "                \"event_name_1\",\n",
    "                \"event_type_1\",\n",
    "                \"event_name_2\",\n",
    "                \"event_type_2\",\n",
    "                \"snap_CA\",\n",
    "                \"snap_TX\",\n",
    "                \"snap_WI\",\n",
    "                \"sell_price\",\n",
    "                # demand features\n",
    "                \"shift_t28\",\n",
    "                \"shift_t29\",\n",
    "                \"shift_t30\",\n",
    "                # std\n",
    "                \"rolling_std_t7\",\n",
    "                \"rolling_std_t30\",\n",
    "                \"rolling_std_t60\",\n",
    "                \"rolling_std_t90\",\n",
    "                \"rolling_std_t180\",\n",
    "                # mean\n",
    "                \"rolling_mean_t7\",\n",
    "                \"rolling_mean_t30\",\n",
    "                \"rolling_mean_t60\",\n",
    "                \"rolling_mean_t90\",\n",
    "                \"rolling_mean_t180\",\n",
    "                # min\n",
    "                \"rolling_min_t7\",\n",
    "                \"rolling_min_t30\",\n",
    "                \"rolling_min_t60\",\n",
    "                # max\n",
    "                \"rolling_max_t7\",\n",
    "                \"rolling_max_t30\",\n",
    "                \"rolling_max_t60\",\n",
    "                # others\n",
    "                \"rolling_skew_t30\",\n",
    "                \"rolling_kurt_t30\",\n",
    "                # price features\n",
    "                \"price_change_t1\",\n",
    "                \"price_change_t365\",\n",
    "                \"rolling_price_std_t7\",\n",
    "                \"rolling_price_std_t30\",\n",
    "                # time features\n",
    "                \"year\",\n",
    "                \"quarter\",\n",
    "                \"month\",\n",
    "                \"week\",\n",
    "                \"day\",\n",
    "                \"dayofweek\",\n",
    "                \"is_weekend\",\n",
    "            ]\n",
    "\n",
    "for featName, featFunc in dateFeatures.items():\n",
    "    if featName in ds.columns:\n",
    "        ds[featName] = ds[featName].astype(\"int16\")\n",
    "    else:\n",
    "        ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_28</th>\n",
       "      <th>rmean_7_7</th>\n",
       "      <th>rmean_28_7</th>\n",
       "      <th>rmean_7_28</th>\n",
       "      <th>rmean_28_28</th>\n",
       "      <th>week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>mday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>3.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>3.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-07</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>3.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_251</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-10-06</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  store_id  cat_id  \\\n",
       "0  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "1  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "2  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "3  HOBBIES_1_004_CA_1_evaluation        3        0         0       0   \n",
       "4  HOBBIES_1_004_CA_1_evaluation        3        0         0       0   \n",
       "\n",
       "   state_id      d  sales       date  wm_yr_wk  ...  sell_price  lag_7  \\\n",
       "0         0  d_250    0.0 2011-10-05     11136  ...        3.97    NaN   \n",
       "1         0  d_251    0.0 2011-10-06     11136  ...        3.97    NaN   \n",
       "2         0  d_252    0.0 2011-10-07     11136  ...        3.97    NaN   \n",
       "3         0  d_250    0.0 2011-10-05     11136  ...        4.34    NaN   \n",
       "4         0  d_251    4.0 2011-10-06     11136  ...        4.34    NaN   \n",
       "\n",
       "   lag_28  rmean_7_7  rmean_28_7  rmean_7_28  rmean_28_28  week  quarter  mday  \n",
       "0     NaN        NaN         NaN         NaN          NaN    40       40    40  \n",
       "1     NaN        NaN         NaN         NaN          NaN    40       40    40  \n",
       "2     NaN        NaN         NaN         NaN          NaN    40       40    40  \n",
       "3     NaN        NaN         NaN         NaN          NaN    40       40    40  \n",
       "4     NaN        NaN         NaN         NaN          NaN    40       40    40  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42372682 entries, 0 to 42372681\n",
      "Data columns (total 31 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   id            object        \n",
      " 1   item_id       int16         \n",
      " 2   dept_id       int16         \n",
      " 3   store_id      int16         \n",
      " 4   cat_id        int16         \n",
      " 5   state_id      int16         \n",
      " 6   d             object        \n",
      " 7   sales         float32       \n",
      " 8   date          datetime64[ns]\n",
      " 9   wm_yr_wk      int16         \n",
      " 10  weekday       int16         \n",
      " 11  wday          int16         \n",
      " 12  month         int16         \n",
      " 13  year          int16         \n",
      " 14  event_name_1  int16         \n",
      " 15  event_type_1  int16         \n",
      " 16  event_name_2  int16         \n",
      " 17  event_type_2  int16         \n",
      " 18  snap_CA       float32       \n",
      " 19  snap_TX       float32       \n",
      " 20  snap_WI       float32       \n",
      " 21  sell_price    float32       \n",
      " 22  lag_7         float32       \n",
      " 23  lag_28        float32       \n",
      " 24  rmean_7_7     float64       \n",
      " 25  rmean_28_7    float64       \n",
      " 26  rmean_7_28    float64       \n",
      " 27  rmean_28_28   float64       \n",
      " 28  week          int16         \n",
      " 29  quarter       int16         \n",
      " 30  mday          int16         \n",
      "dtypes: datetime64[ns](1), float32(7), float64(4), int16(17), object(2)\n",
      "memory usage: 4.7+ GB\n"
     ]
    }
   ],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28', 'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28', 'week', 'quarter', 'mday']\n"
     ]
    }
   ],
   "source": [
    "print(ds.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28', 'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28', 'week', 'quarter', 'mday']\n"
     ]
    }
   ],
   "source": [
    "# NaN を削除\n",
    "ds.dropna(inplace=True)\n",
    "\n",
    "# Define columns that need to be removed\n",
    "unusedCols = [\"id\",\"sales\", \"date\" ,\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "print(ds.columns.tolist())\n",
    "trainCols = ds.columns[~ds.columns.isin(unusedCols)]\n",
    "X_train = ds[trainCols]\n",
    "y_train = ds[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "# Define categorical features\n",
    "catFeats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + \\\n",
    "           [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "\n",
    "validInds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "trainInds = np.setdiff1d(X_train.index.values, validInds)\n",
    "\n",
    "trainData = lgb.Dataset(X_train.loc[trainInds], label = y_train.loc[trainInds], \n",
    "                        categorical_feature = catFeats, free_raw_data = False)\n",
    "validData = lgb.Dataset(X_train.loc[validInds], label = y_train.loc[validInds],\n",
    "                        categorical_feature = catFeats, free_raw_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del ds, X_train, y_train, validInds, trainInds ; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"poisson\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"force_row_wise\": True,\n",
    "    \"learning_rate\": 0.075,\n",
    "    \"sub_row\": 0.75,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    'verbosity': 1, # ここで出力の詳細を制御します\n",
    "    # 'num_iterations': 1200,\n",
    "    'num_iterations': 2400,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"verbose\": -1 # verbose_evalと同様に出力を抑制・制御する場合の設定\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC_User\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] verbosity is set=1, verbose=-1 will be ignored. Current value: verbosity=1\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] verbosity is set=1, verbose=-1 will be ignored. Current value: verbosity=1\n",
      "[LightGBM] [Info] Total Bins 4671\n",
      "[LightGBM] [Info] Number of data points in the train set: 38695732, number of used features: 25\n",
      "[LightGBM] [Warning] verbosity is set=1, verbose=-1 will be ignored. Current value: verbosity=1\n",
      "[LightGBM] [Info] Start training from score 0.312462\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM model\n",
    "m_lgb = lgb.train(params, trainData, valid_sets=[validData])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1b167774a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "m_lgb.save_model(\"model.lgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もらったやつ\n",
    "# Last day used for training\n",
    "# trLast = 1913\n",
    "trLast = 1927\n",
    "# Maximum lag day\n",
    "maxLags = 57\n",
    "\n",
    "# Create dataset for predictions\n",
    "def create_ds():\n",
    "    \n",
    "    startDay = trLast - maxLags\n",
    "    \n",
    "    numCols = [f\"d_{day}\" for day in range(startDay, trLast + 1)]\n",
    "    catCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    \n",
    "    dtype = {numCol:\"float32\" for numCol in numCols} \n",
    "    dtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n",
    "    \n",
    "    ds = pd.read_csv(\"data/trainset.csv\", \n",
    "                     usecols = catCols + numCols, dtype = dtype)\n",
    "    \n",
    "    for col in catCols:\n",
    "        if col != \"id\":\n",
    "            ds[col] = ds[col].cat.codes.astype(\"int16\")\n",
    "            ds[col] -= ds[col].min()\n",
    "    \n",
    "    for day in range(trLast + 1, trLast+ 28 +1):\n",
    "        ds[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    ds = pd.melt(ds,\n",
    "                 id_vars = catCols,\n",
    "                 value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n",
    "                 var_name = \"d\",\n",
    "                 value_name = \"sales\")\n",
    "    \n",
    "    ds = ds.merge(calendar, on = \"d\", copy = False)\n",
    "    ds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_features(ds):          \n",
    "    dayLags = [7, 28]\n",
    "    lagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\n",
    "    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "        ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n",
    "\n",
    "    windows = [7, 28]\n",
    "    for window in windows:\n",
    "        for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "            ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())\n",
    "          \n",
    "    dateFeatures = {\n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"week\",               # 修正点：週番号はisocalendar().weekを使用\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\"\n",
    "    }\n",
    "\n",
    "    for featName, featFunc in dateFeatures.items():\n",
    "        if featName in ds.columns:\n",
    "            ds[featName] = ds[featName].astype(\"int16\")\n",
    "        else:\n",
    "            if featFunc == \"week\":\n",
    "                # isocalendar()で週番号を取得\n",
    "                ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")\n",
    "            else:\n",
    "                ds[featName] = getattr(ds[\"date\"].dt, featFunc).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測結果が 'submission.csv' に保存されました。\n"
     ]
    }
   ],
   "source": [
    "#もらったやつ\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 提出ファイルを読み込み\n",
    "submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "\n",
    "# 予測開始日と列名の定義\n",
    "fday = pd.to_datetime(\"2016-05-09\")  # d_1928に対応\n",
    "prediction_days = 14  # 予測日数\n",
    "target_days = [f\"d_{1928 + i}\" for i in range(prediction_days)]  # d_1928からd_1941の列名\n",
    "\n",
    "# 予測用のデータセットを作成\n",
    "te = create_ds()\n",
    "\n",
    "# 各日付について予測を行い、結果を埋め込む\n",
    "for tdelta in range(prediction_days):\n",
    "    day = fday + timedelta(days=tdelta)\n",
    "    tst = te[(te['date'] >= day - timedelta(days=maxLags)) & (te['date'] <= day)].copy()\n",
    "    create_features(tst)  # 特徴量の生成\n",
    "    tst = tst.loc[tst['date'] == day, trainCols]  # 当日分のデータを抽出\n",
    "    \n",
    "    # LightGBMモデルで予測し、結果を保存\n",
    "    te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst)\n",
    "    # 予測結果を四捨五入して整数に変換\n",
    "    # te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst).round()\n",
    "\n",
    "# 提出ファイルの0の箇所に予測結果を埋め込む\n",
    "for i, day_col in enumerate(target_days):\n",
    "    pred_data = te.loc[te['date'] == fday + timedelta(days=i), [\"id\", \"sales\"]]\n",
    "    submit_df[day_col] = submit_df[\"id\"].map(pred_data.set_index(\"id\")[\"sales\"])\n",
    "\n",
    "# 最終提出ファイルとして保存\n",
    "submit_df.to_csv(\"submission_basic_1201.csv\", index=False)\n",
    "print(\"予測結果が 'submission.csv' に保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.028 0.3333333333333333\n",
      "1 1.023 0.3333333333333333\n",
      "2 1.018 0.3333333333333333\n",
      "予測結果が 'submission2.csv' に保存されました。\n"
     ]
    }
   ],
   "source": [
    "#もらったやつを変更\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 提出ファイルを読み込み\n",
    "submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "\n",
    "# 予測開始日と列名の定義\n",
    "fday = pd.to_datetime(\"2016-05-09\")  # d_1928に対応\n",
    "prediction_days = 14  # 予測日数\n",
    "target_days = [f\"d_{1928 + i}\" for i in range(prediction_days)]  # d_1928からd_1941の列名\n",
    "\n",
    "#変数\n",
    "alphas = [1.028, 1.023, 1.018]\n",
    "weights = [1 / len(alphas)] * len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "# 予測用のデータセットを作成\n",
    "te = create_ds()\n",
    "\n",
    "# 各日付について予測を行い、結果を埋め込む\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "    te = create_ds()\n",
    "    cols = [f\"d_{i}\" for i in range(1928, 1940)]\n",
    "    for tdelta in range(prediction_days):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        tst = te[(te['date'] >= day - timedelta(days=maxLags)) & (te['date'] <= day)].copy()\n",
    "        create_features(tst)  # 特徴量の生成\n",
    "        tst = tst.loc[tst['date'] == day, trainCols]  # 当日分のデータを抽出\n",
    "        \n",
    "        # LightGBMモデルで予測し、結果を保存\n",
    "        alpha_lab = alpha * m_lgb.predict(tst)\n",
    "        \n",
    "        te.loc[te['date'] == day, \"sales\"] = alpha_lab.round()\n",
    "        # 予測結果を四捨五入して整数に変換\n",
    "        # te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst).round()\n",
    "\n",
    "    # 提出ファイルの0の箇所に予測結果を埋め込む\n",
    "    for i, day_col in enumerate(target_days):\n",
    "        pred_data = te.loc[te['date'] == fday + timedelta(days=i), [\"id\", \"sales\"]]\n",
    "        submit_df[day_col] = submit_df[\"id\"].map(pred_data.set_index(\"id\")[\"sales\"])\n",
    "\n",
    "    submit_df.to_csv(f\"submin_1201_alpha{icount}.csv\", index=False)\n",
    "        \n",
    "    if icount == 0:\n",
    "        sub = submit_df\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += submit_df[cols] * weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "# 最終提出ファイルとして保存\n",
    "submit_df.to_csv(\"submission2_dataengineer_1201.csv\", index=False)\n",
    "print(\"予測結果が 'submission2.csv' に保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
