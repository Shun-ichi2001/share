{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from  datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the correct data type for each column in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *calendar.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  wm_yr_wk  weekday  wday  month  year    d  event_name_1  \\\n",
       "0 2011-01-29     11101        2     1      1  2011  d_1             0   \n",
       "1 2011-01-30     11101        3     2      1  2011  d_2             0   \n",
       "2 2011-01-31     11101        1     3      1  2011  d_3             0   \n",
       "3 2011-02-01     11101        5     4      2  2011  d_4             0   \n",
       "4 2011-02-02     11101        6     5      2  2011  d_5             0   \n",
       "\n",
       "   event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0             0             0             0      0.0      0.0      0.0  \n",
       "1             0             0             0      0.0      0.0      0.0  \n",
       "2             0             0             0      0.0      0.0      0.0  \n",
       "3             0             0             0      1.0      1.0      0.0  \n",
       "4             0             0             0      1.0      0.0      1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct data types for \"calendar.csv\"\n",
    "calendarDTypes = {\"event_name_1\": \"category\", \n",
    "                  \"event_name_2\": \"category\", \n",
    "                  \"event_type_1\": \"category\", \n",
    "                  \"event_type_2\": \"category\", \n",
    "                  \"weekday\": \"category\", \n",
    "                  'wm_yr_wk': 'int16', \n",
    "                  \"wday\": \"int16\",\n",
    "                  \"month\": \"int16\", \n",
    "                  \"year\": \"int16\", \n",
    "                  \"snap_CA\": \"float32\", \n",
    "                  'snap_TX': 'float32', \n",
    "                  'snap_WI': 'float32' }\n",
    "\n",
    "# Read csv file\n",
    "# ここは変える\n",
    "calendar = pd.read_csv(\"data/calendar.csv\", \n",
    "                       dtype = calendarDTypes)\n",
    "\n",
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col, colDType in calendarDTypes.items():\n",
    "    if colDType == \"category\":\n",
    "        calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n",
    "        calendar[col] -= calendar[col].min()\n",
    "\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sell_prices.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id  item_id  wm_yr_wk  sell_price\n",
       "0         0        0     11325        9.58\n",
       "1         0        0     11326        9.58\n",
       "2         0        0     11327        8.26\n",
       "3         0        0     11328        8.26\n",
       "4         0        0     11329        8.26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct data types for \"sell_prices.csv\"\n",
    "priceDTypes = {\"store_id\": \"category\", \n",
    "               \"item_id\": \"category\", \n",
    "               \"wm_yr_wk\": \"int16\",\n",
    "               \"sell_price\":\"float32\"}\n",
    "\n",
    "# Read csv file\n",
    "prices = pd.read_csv(\"data/sell_prices.csv\", \n",
    "                     dtype = priceDTypes)\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col, colDType in priceDTypes.items():\n",
    "    if colDType == \"category\":\n",
    "        prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "        prices[col] -= prices[col].min()\n",
    "        \n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *sales_train_validation.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *train.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-10-05</td>\n",
       "      <td>11136</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  store_id  cat_id  \\\n",
       "0  HOBBIES_1_002_CA_1_evaluation        1        0         0       0   \n",
       "1  HOBBIES_1_004_CA_1_evaluation        3        0         0       0   \n",
       "2  HOBBIES_1_005_CA_1_evaluation        4        0         0       0   \n",
       "3  HOBBIES_1_008_CA_1_evaluation        7        0         0       0   \n",
       "4  HOBBIES_1_009_CA_1_evaluation        8        0         0       0   \n",
       "\n",
       "   state_id      d  sales       date  wm_yr_wk  ...  month  year  \\\n",
       "0         0  d_250    0.0 2011-10-05     11136  ...     10  2011   \n",
       "1         0  d_250    0.0 2011-10-05     11136  ...     10  2011   \n",
       "2         0  d_250    0.0 2011-10-05     11136  ...     10  2011   \n",
       "3         0  d_250    1.0 2011-10-05     11136  ...     10  2011   \n",
       "4         0  d_250    2.0 2011-10-05     11136  ...     10  2011   \n",
       "\n",
       "   event_name_1  event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  \\\n",
       "0             0             0             0             0      1.0      1.0   \n",
       "1             0             0             0             0      1.0      1.0   \n",
       "2             0             0             0             0      1.0      1.0   \n",
       "3             0             0             0             0      1.0      1.0   \n",
       "4             0             0             0             0      1.0      1.0   \n",
       "\n",
       "   snap_WI  sell_price  \n",
       "0      1.0        3.97  \n",
       "1      1.0        4.34  \n",
       "2      1.0        2.48  \n",
       "3      1.0        0.50  \n",
       "4      1.0        1.77  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstDay = 250\n",
    "lastDay = 1913\n",
    "\n",
    "# Use x sales days (columns) for training\n",
    "numCols = [f\"d_{day}\" for day in range(firstDay, lastDay+1)]\n",
    "\n",
    "# Define all categorical columns\n",
    "catCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "\n",
    "# Define the correct data types for \"sales_train_validation.csv\"\n",
    "dtype = {numCol: \"float32\" for numCol in numCols} \n",
    "dtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n",
    "\n",
    "# Read csv file\n",
    "# ここ変える\n",
    "ds = pd.read_csv(\"data/trainset.csv\", \n",
    "                 usecols = catCols + numCols, dtype = dtype)\n",
    "\n",
    "# Transform categorical features into integers\n",
    "for col in catCols:\n",
    "    if col != \"id\":\n",
    "        ds[col] = ds[col].cat.codes.astype(\"int16\")\n",
    "        ds[col] -= ds[col].min()\n",
    "        \n",
    "ds = pd.melt(ds,\n",
    "             id_vars = catCols,\n",
    "             value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n",
    "             var_name = \"d\",\n",
    "             value_name = \"sales\")\n",
    "\n",
    "# Merge \"ds\" with \"calendar\" and \"prices\" dataframe\n",
    "ds = ds.merge(calendar, on = \"d\", copy = False)\n",
    "ds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加した特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    submission = submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "    print(\"submission shape:\", submission.shape)\n",
    "\n",
    "    # calendar shape: (1969, 14)\n",
    "    # sell_prices shape: (6841121, 4)\n",
    "    # sales_train_val shape: (30490, 1919)\n",
    "    # submission shape: (60980, 29)\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission shape: (30490, 20)\n"
     ]
    }
   ],
   "source": [
    "calendar\n",
    "sales = ds\n",
    "prices\n",
    "submission = read_data()\n",
    "\n",
    "NUM_ITEMS = ds.shape[0]  # 30490\n",
    "DAYS_PRED = submission.shape[1] - 1  # 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar = encode_categorical(\n",
    "    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "sales = encode_categorical(\n",
    "    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "prices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num(ser):\n",
    "    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n",
    "\n",
    "\n",
    "def reshape_sales(sales, submission, d_thresh=0, verbose=True):\n",
    "    # melt sales data, get it ready for training\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    # get product table.\n",
    "    product = sales[id_columns]\n",
    "\n",
    "    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n",
    "    sales = reduce_mem_usage(sales)\n",
    "\n",
    "    # separate test dataframes.\n",
    "    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    # change column names.\n",
    "    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n",
    "    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "\n",
    "    # merge with product table\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n",
    "    vals = vals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"validation\")\n",
    "        display(vals)\n",
    "\n",
    "        print(\"evaluation\")\n",
    "        display(evals)\n",
    "\n",
    "    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    sales[\"part\"] = \"train\"\n",
    "    vals[\"part\"] = \"validation\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "\n",
    "    data = pd.concat([sales, vals, evals], axis=0)\n",
    "\n",
    "    del sales, vals, evals\n",
    "\n",
    "    data[\"d\"] = extract_num(data[\"d\"])\n",
    "    data = data[data[\"d\"] >= d_thresh]\n",
    "\n",
    "    # delete evaluation for now.\n",
    "    data = data[data[\"part\"] != \"evaluation\"]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data\")\n",
    "        display(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_calendar(data, calendar):\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "\n",
    "def merge_prices(data, prices):\n",
    "    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_demand_features(df):\n",
    "    for diff in [0, 1, 2]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60, 90, 180]:\n",
    "        df[f\"rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60, 90, 180]:\n",
    "        df[f\"rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60]:\n",
    "        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n",
    "        )\n",
    "\n",
    "    for window in [7, 30, 60]:\n",
    "        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n",
    "        )\n",
    "\n",
    "    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n",
    "    )\n",
    "    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_features(df):\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n",
    "\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    attrs = [\n",
    "        \"year\",\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 45.78 MB\n",
      "Memory usage after optimization is: 17.43 MB\n",
      "Decreased by 61.9%\n",
      "validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>d_1917</th>\n",
       "      <th>d_1918</th>\n",
       "      <th>d_1919</th>\n",
       "      <th>d_1920</th>\n",
       "      <th>d_1921</th>\n",
       "      <th>d_1922</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1928</th>\n",
       "      <th>d_1929</th>\n",
       "      <th>d_1930</th>\n",
       "      <th>d_1931</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, d_1914, d_1915, d_1916, d_1917, d_1918, d_1919, d_1920, d_1921, d_1922, d_1923, d_1924, d_1925, d_1926, d_1927, d_1928, d_1929, d_1930, d_1931, d_1932, item_id, dept_id, cat_id, store_id, state_id]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d_1942</th>\n",
       "      <th>d_1943</th>\n",
       "      <th>d_1944</th>\n",
       "      <th>d_1945</th>\n",
       "      <th>d_1946</th>\n",
       "      <th>d_1947</th>\n",
       "      <th>d_1948</th>\n",
       "      <th>d_1949</th>\n",
       "      <th>d_1950</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1956</th>\n",
       "      <th>d_1957</th>\n",
       "      <th>d_1958</th>\n",
       "      <th>d_1959</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id         d_1942     d_1943   d_1944  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                              ...            ...        ...      ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
       "30486    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
       "30487    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
       "30488    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
       "30489    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
       "\n",
       "      d_1945 d_1946  d_1947  d_1948  d_1949  d_1950  ...  d_1956  d_1957  \\\n",
       "0       CA_1     CA       0       0       0       0  ...       0       0   \n",
       "1       CA_1     CA       0       0       0       0  ...       0       0   \n",
       "2       CA_1     CA       0       0       0       0  ...       0       0   \n",
       "3       CA_1     CA       0       0       0       0  ...       0       0   \n",
       "4       CA_1     CA       0       0       0       0  ...       0       0   \n",
       "...      ...    ...     ...     ...     ...     ...  ...     ...     ...   \n",
       "30485   WI_3     WI       0       0       0       0  ...       0       0   \n",
       "30486   WI_3     WI       0       0       0       0  ...       0       0   \n",
       "30487   WI_3     WI       0       0       0       0  ...       0       0   \n",
       "30488   WI_3     WI       0       0       0       0  ...       0       0   \n",
       "30489   WI_3     WI       0       0       0       0  ...       0       0   \n",
       "\n",
       "       d_1958  d_1959  d_1960  item_id  dept_id  cat_id  store_id  state_id  \n",
       "0           0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "1           0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "2           0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "3           0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "4           0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "...       ...     ...     ...      ...      ...     ...       ...       ...  \n",
       "30485       0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "30486       0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "30487       0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "30488       0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "30489       0       0       0      NaN      NaN     NaN       NaN       NaN  \n",
       "\n",
       "[30490 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(reshaped_chunks)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Main process\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m data \u001b[38;5;241m=\u001b[39m reshape_sales_in_chunks(sales, submission, d_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1927\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m365\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sales\n\u001b[0;32m     57\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[1;32mIn[29], line 48\u001b[0m, in \u001b[0;36mreshape_sales_in_chunks\u001b[1;34m(sales, submission, d_thresh)\u001b[0m\n\u001b[0;32m     46\u001b[0m reshaped_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m---> 48\u001b[0m     reshaped_chunk \u001b[38;5;241m=\u001b[39m reshape_sales(chunk, submission, d_thresh)\n\u001b[0;32m     49\u001b[0m     reshaped_chunks\u001b[38;5;241m.\u001b[39mappend(reshaped_chunk)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m chunk\n",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m, in \u001b[0;36mreshape_sales\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     43\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([sales, vals, evals], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sales, vals, evals\n\u001b[1;32m---> 47\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_num(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m d_thresh]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# delete evaluation for now.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m, in \u001b[0;36mextract_num\u001b[1;34m(ser)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract numerical part from string and handle NaN values.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m ser \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace NaN with a default value, e.g., \"0\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask import dataframe as dd\n",
    "\n",
    "# Reduce memory usage function\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object and str(col_type)[:3] != 'dat':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif col_type == object:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "    print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n",
    "    return df\n",
    "\n",
    "# Function to handle NaN values during extraction\n",
    "def extract_num(ser):\n",
    "    \"\"\"Extract numerical part from string and handle NaN values.\"\"\"\n",
    "    ser = ser.fillna(\"0\")  # Replace NaN with a default value, e.g., \"0\"\n",
    "    return ser.astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "# Process reshape_sales in chunks\n",
    "def reshape_sales_in_chunks(sales, submission, d_thresh):\n",
    "    chunk_size = 100000  # Adjust based on your system's memory capacity\n",
    "    chunks = [sales.iloc[i:i+chunk_size] for i in range(0, len(sales), chunk_size)]\n",
    "    reshaped_chunks = []\n",
    "    for chunk in chunks:\n",
    "        reshaped_chunk = reshape_sales(chunk, submission, d_thresh)\n",
    "        reshaped_chunks.append(reshaped_chunk)\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    return pd.concat(reshaped_chunks)\n",
    "\n",
    "# Main process\n",
    "data = reshape_sales_in_chunks(sales, submission, d_thresh=1927 - int(365 * 2))\n",
    "del sales\n",
    "gc.collect()\n",
    "\n",
    "# Convert calendar to Dask DataFrame for efficient merging\n",
    "calendar[\"d\"] = extract_num(calendar[\"d\"])  # Extract numerical values, handling NaN\n",
    "data = merge_calendar(data, calendar)\n",
    "del calendar\n",
    "gc.collect()\n",
    "\n",
    "data = merge_prices(data, prices)\n",
    "del prices\n",
    "gc.collect()\n",
    "\n",
    "data = reduce_mem_usage(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m add_demand_features(data)\u001b[38;5;241m.\u001b[39mpipe(reduce_mem_usage)\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m add_price_features(data)\u001b[38;5;241m.\u001b[39mpipe(reduce_mem_usage)\n\u001b[0;32m      3\u001b[0m dt_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")\n",
    "\n",
    "print(\"start date:\", data[dt_col].min())\n",
    "print(\"end date:\", data[dt_col].max())\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTimeSeriesSplitter:\n",
    "    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n",
    "        self.n_splits = n_splits\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.day_col = day_col\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        SEC_IN_DAY = 3600 * 24\n",
    "        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n",
    "        duration = sec.max()\n",
    "\n",
    "        train_sec = self.train_days * SEC_IN_DAY\n",
    "        test_sec = self.test_days * SEC_IN_DAY\n",
    "        total_sec = test_sec + train_sec\n",
    "\n",
    "        if self.n_splits == 1:\n",
    "            train_start = duration - total_sec\n",
    "            train_end = train_start + train_sec\n",
    "\n",
    "            train_mask = (sec >= train_start) & (sec < train_end)\n",
    "            test_mask = sec >= train_end\n",
    "\n",
    "            yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "        else:\n",
    "            # step = (duration - total_sec) / (self.n_splits - 1)\n",
    "            step = DAYS_PRED * SEC_IN_DAY\n",
    "\n",
    "            for idx in range(self.n_splits):\n",
    "                # train_start = idx * step\n",
    "                shift = (self.n_splits - (idx + 1)) * step\n",
    "                train_start = duration - total_sec - shift\n",
    "                train_end = train_start + train_sec\n",
    "                test_end = train_end + test_sec\n",
    "\n",
    "                train_mask = (sec > train_start) & (sec <= train_end)\n",
    "\n",
    "                if idx == self.n_splits - 1:\n",
    "                    test_mask = sec > train_end\n",
    "                else:\n",
    "                    test_mask = (sec > train_end) & (sec <= test_end)\n",
    "\n",
    "                yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_col = \"d\"\n",
    "cv_params = {\n",
    "    \"n_splits\": 3,\n",
    "    \"train_days\": int(365 * 1.5),\n",
    "    \"test_days\": DAYS_PRED,\n",
    "    \"day_col\": day_col,\n",
    "}\n",
    "cv = CustomTimeSeriesSplitter(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cv_days(cv, X, dt_col, day_col):\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X)):\n",
    "        print(f\"----- Fold: ({ii + 1} / {cv.n_splits}) -----\")\n",
    "        tr_start = X.iloc[tr][dt_col].min()\n",
    "        tr_end = X.iloc[tr][dt_col].max()\n",
    "        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n",
    "\n",
    "        tt_start = X.iloc[tt][dt_col].min()\n",
    "        tt_end = X.iloc[tt][dt_col].max()\n",
    "        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"start\": [tr_start, tt_start],\n",
    "                \"end\": [tr_end, tt_end],\n",
    "                \"days\": [tr_days, tt_days],\n",
    "            },\n",
    "            index=[\"train\", \"test\"],\n",
    "        )\n",
    "\n",
    "        display(df)\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, dt_col, lw=10):\n",
    "    n_splits = cv.get_n_splits()\n",
    "    _, ax = plt.subplots(figsize=(20, n_splits))\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            X[dt_col],\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=plt.cm.coolwarm,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    MIDDLE = 15\n",
    "    LARGE = 20\n",
    "    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n",
    "    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n",
    "    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n",
    "    ax.set_yticks(np.arange(n_splits) + 0.5)\n",
    "    ax.set_yticklabels(list(range(n_splits)))\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\n",
    "show_cv_days(cv, sample, dt_col, day_col)\n",
    "plot_cv_indices(cv, sample, dt_col)\n",
    "\n",
    "del sample\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayLags = [7, 28]\n",
    "lagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\n",
    "for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "    ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n",
    "    \n",
    "windows = [7, 28]\n",
    "for window in windows:\n",
    "    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "        ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFeatures = {\"wday\": \"weekday\",\n",
    "                \"week\": \"weekofyear\",\n",
    "                \"month\": \"month\",\n",
    "                \"quarter\": \"quarter\",\n",
    "                \"year\": \"year\",\n",
    "                \"mday\": \"day\"}\n",
    "\n",
    "for featName, featFunc in dateFeatures.items():\n",
    "    if featName in ds.columns:\n",
    "        ds[featName] = ds[featName].astype(\"int16\")\n",
    "    else:\n",
    "        ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFeatures = {\"item_id\",\n",
    "                \"dept_id\",\n",
    "                \"cat_id\",\n",
    "                \"store_id\",\n",
    "                \"state_id\",\n",
    "                \"event_name_1\",\n",
    "                \"event_type_1\",\n",
    "                \"event_name_2\",\n",
    "                \"event_type_2\",\n",
    "                \"snap_CA\",\n",
    "                \"snap_TX\",\n",
    "                \"snap_WI\",\n",
    "                \"sell_price\",\n",
    "                # demand features\n",
    "                \"shift_t28\",\n",
    "                \"shift_t29\",\n",
    "                \"shift_t30\",\n",
    "                # std\n",
    "                \"rolling_std_t7\",\n",
    "                \"rolling_std_t30\",\n",
    "                \"rolling_std_t60\",\n",
    "                \"rolling_std_t90\",\n",
    "                \"rolling_std_t180\",\n",
    "                # mean\n",
    "                \"rolling_mean_t7\",\n",
    "                \"rolling_mean_t30\",\n",
    "                \"rolling_mean_t60\",\n",
    "                \"rolling_mean_t90\",\n",
    "                \"rolling_mean_t180\",\n",
    "                # min\n",
    "                \"rolling_min_t7\",\n",
    "                \"rolling_min_t30\",\n",
    "                \"rolling_min_t60\",\n",
    "                # max\n",
    "                \"rolling_max_t7\",\n",
    "                \"rolling_max_t30\",\n",
    "                \"rolling_max_t60\",\n",
    "                # others\n",
    "                \"rolling_skew_t30\",\n",
    "                \"rolling_kurt_t30\",\n",
    "                # price features\n",
    "                \"price_change_t1\",\n",
    "                \"price_change_t365\",\n",
    "                \"rolling_price_std_t7\",\n",
    "                \"rolling_price_std_t30\",\n",
    "                # time features\n",
    "                \"year\",\n",
    "                \"quarter\",\n",
    "                \"month\",\n",
    "                \"week\",\n",
    "                \"day\",\n",
    "                \"dayofweek\",\n",
    "                \"is_weekend\",\n",
    "            ]\n",
    "\n",
    "for featName, featFunc in dateFeatures.items():\n",
    "    if featName in ds.columns:\n",
    "        ds[featName] = ds[featName].astype(\"int16\")\n",
    "    else:\n",
    "        ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN を削除\n",
    "ds.dropna(inplace=True)\n",
    "\n",
    "# Define columns that need to be removed\n",
    "unusedCols = [\"id\",\"sales\", \"date\" ,\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "print(ds.columns.tolist())\n",
    "trainCols = ds.columns[~ds.columns.isin(unusedCols)]\n",
    "X_train = ds[trainCols]\n",
    "y_train = ds[\"sales\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "# Define categorical features\n",
    "catFeats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + \\\n",
    "           [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "\n",
    "validInds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "trainInds = np.setdiff1d(X_train.index.values, validInds)\n",
    "\n",
    "trainData = lgb.Dataset(X_train.loc[trainInds], label = y_train.loc[trainInds], \n",
    "                        categorical_feature = catFeats, free_raw_data = False)\n",
    "validData = lgb.Dataset(X_train.loc[validInds], label = y_train.loc[validInds],\n",
    "                        categorical_feature = catFeats, free_raw_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds, X_train, y_train, validInds, trainInds ; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"poisson\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"force_row_wise\": True,\n",
    "    \"learning_rate\": 0.075,\n",
    "    \"sub_row\": 0.75,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 0.1,\n",
    "    'verbosity': 1, # ここで出力の詳細を制御します\n",
    "    # 'num_iterations': 1200,\n",
    "    'num_iterations': 2400,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"verbose\": -1 # verbose_evalと同様に出力を抑制・制御する場合の設定\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "m_lgb = lgb.train(params, trainData, valid_sets=[validData])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "m_lgb.save_model(\"model.lgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もらったやつ\n",
    "# Last day used for training\n",
    "# trLast = 1913\n",
    "trLast = 1927\n",
    "# Maximum lag day\n",
    "maxLags = 57\n",
    "\n",
    "# Create dataset for predictions\n",
    "def create_ds():\n",
    "    \n",
    "    startDay = trLast - maxLags\n",
    "    \n",
    "    numCols = [f\"d_{day}\" for day in range(startDay, trLast + 1)]\n",
    "    catCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    \n",
    "    dtype = {numCol:\"float32\" for numCol in numCols} \n",
    "    dtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n",
    "    \n",
    "    ds = pd.read_csv(\"data/trainset.csv\", \n",
    "                     usecols = catCols + numCols, dtype = dtype)\n",
    "    \n",
    "    for col in catCols:\n",
    "        if col != \"id\":\n",
    "            ds[col] = ds[col].cat.codes.astype(\"int16\")\n",
    "            ds[col] -= ds[col].min()\n",
    "    \n",
    "    for day in range(trLast + 1, trLast+ 28 +1):\n",
    "        ds[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    ds = pd.melt(ds,\n",
    "                 id_vars = catCols,\n",
    "                 value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n",
    "                 var_name = \"d\",\n",
    "                 value_name = \"sales\")\n",
    "    \n",
    "    ds = ds.merge(calendar, on = \"d\", copy = False)\n",
    "    ds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_features(ds):          \n",
    "    dayLags = [7, 28]\n",
    "    lagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\n",
    "    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "        ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n",
    "\n",
    "    windows = [7, 28]\n",
    "    for window in windows:\n",
    "        for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n",
    "            ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())\n",
    "          \n",
    "    dateFeatures = {\n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"week\",               # 修正点：週番号はisocalendar().weekを使用\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\"\n",
    "    }\n",
    "\n",
    "    for featName, featFunc in dateFeatures.items():\n",
    "        if featName in ds.columns:\n",
    "            ds[featName] = ds[featName].astype(\"int16\")\n",
    "        else:\n",
    "            if featFunc == \"week\":\n",
    "                # isocalendar()で週番号を取得\n",
    "                ds[featName] = ds[\"date\"].dt.isocalendar().week.astype(\"int16\")\n",
    "            else:\n",
    "                ds[featName] = getattr(ds[\"date\"].dt, featFunc).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もらったやつ\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 提出ファイルを読み込み\n",
    "submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "\n",
    "# 予測開始日と列名の定義\n",
    "fday = pd.to_datetime(\"2016-05-09\")  # d_1928に対応\n",
    "prediction_days = 14  # 予測日数\n",
    "target_days = [f\"d_{1928 + i}\" for i in range(prediction_days)]  # d_1928からd_1941の列名\n",
    "\n",
    "# 予測用のデータセットを作成\n",
    "te = create_ds()\n",
    "\n",
    "# 各日付について予測を行い、結果を埋め込む\n",
    "for tdelta in range(prediction_days):\n",
    "    day = fday + timedelta(days=tdelta)\n",
    "    tst = te[(te['date'] >= day - timedelta(days=maxLags)) & (te['date'] <= day)].copy()\n",
    "    create_features(tst)  # 特徴量の生成\n",
    "    tst = tst.loc[tst['date'] == day, trainCols]  # 当日分のデータを抽出\n",
    "    \n",
    "    # LightGBMモデルで予測し、結果を保存\n",
    "    te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst)\n",
    "    # 予測結果を四捨五入して整数に変換\n",
    "    # te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst).round()\n",
    "\n",
    "# 提出ファイルの0の箇所に予測結果を埋め込む\n",
    "for i, day_col in enumerate(target_days):\n",
    "    pred_data = te.loc[te['date'] == fday + timedelta(days=i), [\"id\", \"sales\"]]\n",
    "    submit_df[day_col] = submit_df[\"id\"].map(pred_data.set_index(\"id\")[\"sales\"])\n",
    "\n",
    "# 最終提出ファイルとして保存\n",
    "submit_df.to_csv(\"submission_basic_1201.csv\", index=False)\n",
    "print(\"予測結果が 'submission.csv' に保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#もらったやつを変更\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 提出ファイルを読み込み\n",
    "submit_df = pd.read_csv(\"data/submit.csv\")\n",
    "\n",
    "# 予測開始日と列名の定義\n",
    "fday = pd.to_datetime(\"2016-05-09\")  # d_1928に対応\n",
    "prediction_days = 14  # 予測日数\n",
    "target_days = [f\"d_{1928 + i}\" for i in range(prediction_days)]  # d_1928からd_1941の列名\n",
    "\n",
    "#変数\n",
    "alphas = [1.028, 1.023, 1.018]\n",
    "weights = [1 / len(alphas)] * len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "# 予測用のデータセットを作成\n",
    "te = create_ds()\n",
    "\n",
    "# 各日付について予測を行い、結果を埋め込む\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "    te = create_ds()\n",
    "    cols = [f\"d_{i}\" for i in range(1928, 1940)]\n",
    "    for tdelta in range(prediction_days):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        tst = te[(te['date'] >= day - timedelta(days=maxLags)) & (te['date'] <= day)].copy()\n",
    "        create_features(tst)  # 特徴量の生成\n",
    "        tst = tst.loc[tst['date'] == day, trainCols]  # 当日分のデータを抽出\n",
    "        \n",
    "        # LightGBMモデルで予測し、結果を保存\n",
    "        alpha_lab = alpha * m_lgb.predict(tst)\n",
    "        \n",
    "        te.loc[te['date'] == day, \"sales\"] = alpha_lab.round()\n",
    "        # 予測結果を四捨五入して整数に変換\n",
    "        # te.loc[te['date'] == day, \"sales\"] = m_lgb.predict(tst).round()\n",
    "\n",
    "    # 提出ファイルの0の箇所に予測結果を埋め込む\n",
    "    for i, day_col in enumerate(target_days):\n",
    "        pred_data = te.loc[te['date'] == fday + timedelta(days=i), [\"id\", \"sales\"]]\n",
    "        submit_df[day_col] = submit_df[\"id\"].map(pred_data.set_index(\"id\")[\"sales\"])\n",
    "\n",
    "    submit_df.to_csv(f\"submin_1201_alpha{icount}.csv\", index=False)\n",
    "        \n",
    "    if icount == 0:\n",
    "        sub = submit_df\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += submit_df[cols] * weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "# 最終提出ファイルとして保存\n",
    "submit_df.to_csv(\"submission2_dataengineer_1201.csv\", index=False)\n",
    "print(\"予測結果が 'submission2.csv' に保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
